# 企业开发环境私有云环境

## 本内容你将获得

- centos7 上部署 kubernetes 环境
- centos7 上部署 kuboard

## 环境要求

- ansible
- nfs
- 各服务器时间同步

## centos7 上部署 kubernetes 环境

在服务器中选一个节点下载工具脚本 ezdown

```shell
export release=3.0.0
wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown
chmod +x ./ezdown  #授予可执行权限
```

使用工具脚本安装 kubeasz

```shell
./ezdown -D
安装完成所有文件（kubeasz代码、二进制、离线镜像）均已整理好放入目录/etc/kubeasz
```

集群配置
创建集群配置

```shell
 ezctl new default    # default是集群名称，可以更改
```

修改/etc/kubeasz/clusters/default/hosts,设置集群节点

```shell
[etcd]
Master的ip
[kube_master] #主节点
Master的ip

# work node(s)
[kube_node]  # 工作节点

# [optional] harbor server, a private docker registry
# 'NEW_INSTALL': 'yes' to install a harbor server; 'no' to integrate with existed one
# 'SELF_SIGNED_CERT': 'no' you need put files of certificates named harbor.pem and harbor-key.pem in directory 'down'
[harbor]
#192.168.1.8 HARBOR_DOMAIN="harbor.yourdomain.com" NEW_INSTALL=no SELF_SIGNED_CERT=yes

# [optional] loadbalance for accessing k8s from outside
[ex_lb]
#192.168.1.6 LB_ROLE=backup EX_APISERVER_VIP=192.168.1.250 EX_APISERVER_PORT=8443
#192.168.1.7 LB_ROLE=master EX_APISERVER_VIP=192.168.1.250 EX_APISERVER_PORT=8443

# [optional] ntp server for the cluster
[chrony]
#192.168.1.1

[all:vars]
# --------- Main Variables ---------------
# Cluster container-runtime supported: docker, containerd
CONTAINER_RUNTIME="docker"

# Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn
CLUSTER_NETWORK="flannel"

# Service proxy mode of kube-proxy: 'iptables' or 'ipvs'
PROXY_MODE="ipvs"

# K8S Service CIDR, not overlap with node(host) networking
SERVICE_CIDR="10.68.0.0/16"

# Cluster CIDR (Pod CIDR), not overlap with node(host) networking
CLUSTER_CIDR="172.20.0.0/16"

# NodePort Range
NODE_PORT_RANGE="20000-42767"

# Cluster DNS Domain
CLUSTER_DNS_DOMAIN="cluster.local."

# -------- Additional Variables (don't change the default value right now) ---
# Binaries Directory
bin_dir="/opt/kube/bin"

# Deploy Directory (kubeasz workspace)
base_dir="/etc/kubeasz"

# Directory for a specific cluster
cluster_dir="{{ base_dir }}/clusters/default"

# CA and other components cert/key Directory
ca_dir="/etc/kubernetes/ssl"
```

安装集群

```shell
 ezctl setup default all
```

## centos7 上部署 kuboard

这里部署 kuboard 实际是在 k8s 中跑个 deployment,我们用 yaml 脚本部署 kuboard

### 部署 storageClass

部署 kubarod 之前要先部署存储类(storageClass)，下面是 storageClass 的 yaml 脚本

class.yaml：

```shell
 apiVersion: storage.k8s.io/v1
 kind: StorageClass
 metadata:
 name: managed-nfs-storage
 namespace: alinesno-storageclass
 # or choose another name, must match deployment's env PROVISIONER_NAME'
 provisioner: fuseim.pri/ifs
 parameters:
 archiveOnDelete: "false"
 注意： provisioner 要和 deployment中的环境变量PROVISIONER_NAME 的值相对应
```

rbac.yaml：

```shell
 apiVersion: v1
 kind: ServiceAccount
 metadata:
 name: nfs-client-provisioner
 # replace with namespace where provisioner is deployed
 namespace: alinesno-storageclass
 ---
 kind: ClusterRole
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
 name: nfs-client-provisioner-runner
 namespace: alinesno-storageclass
 rules:
 - apiGroups: [""]
 resources: ["persistentvolumes"]
 verbs: ["get", "list", "watch", "create", "delete"]
 - apiGroups: [""]
 resources: ["persistentvolumeclaims"]
 verbs: ["get", "list", "watch", "update"]
 - apiGroups: ["storage.k8s.io"]
 resources: ["storageclasses"]
 verbs: ["get", "list", "watch"]
 - apiGroups: [""]
 resources: ["events"]
 verbs: ["watch", "create", "update", "patch"]
 - apiGroups: [""]
 resources: ["services", "endpoints"]
 verbs: ["get"]
 - apiGroups: ["extensions"]
 resources: ["podsecuritypolicies"]
 resourceNames: ["nfs-provisioner"]
 verbs: ["use"]
 ---
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
 name: run-nfs-client-provisioner
 namespace: alinesno-storageclass
 subjects:
 - kind: ServiceAccount
 name: nfs-client-provisioner
 # replace with namespace where provisioner is deployed
 namespace: alinesno-storageclass
 roleRef:
 kind: ClusterRole
 name: nfs-client-provisioner-runner
 apiGroup: rbac.authorization.k8s.io
 ---
 kind: Role
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
 name: leader-locking-nfs-client-provisioner
 # replace with namespace where provisioner is deployed
 namespace: alinesno-storageclass
 rules:
 - apiGroups: [""]
 resources: ["endpoints"]
 verbs: ["get", "list", "watch", "create", "update", "patch"]
 ---
 kind: RoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
 name: leader-locking-nfs-client-provisioner
 # replace with namespace where provisioner is deployed
 namespace: alinesno-storageclass
 subjects:
 - kind: ServiceAccount
 name: nfs-client-provisioner
 # replace with namespace where provisioner is deployed
 namespace: alinesno-storageclass
 roleRef:
 kind: Role
 name: leader-locking-nfs-client-provisioner
 apiGroup: rbac.authorization.k8s.io

 ServiceAccount： 服务账户
 ClusterRole： 集群角色
 Role： 命名空间角色
 ***Binding(ClusterRoleBinding, ClusterRole)：给ServiceAccount授予角色权限

 服务账户：服务账户与Namespace绑定，关联一套凭证，存储在Secret中，Pod创建时挂载Secret，从而允许与API Server之间调用
```

```shell
deployment.yaml
 apiVersion: apps/v1
 kind: Deployment
 metadata:
 name: nfs-client-provisioner
 labels:
 app: nfs-client-provisioner
 namespace: alinesno-storageclass
 spec:
 replicas: 1
 strategy:
 type: Recreate
 selector:
 matchLabels:
 app: nfs-client-provisioner
 template:
 metadata:
 labels:
 app: nfs-client-provisioner
 spec:
 serviceAccountName: nfs-client-provisioner
 containers:
        - name: nfs-client-provisioner
 image: easzlab/nfs-subdir-external-provisioner:v4.0.1
 volumeMounts:
 - name: nfs-client-root
 mountPath: /persistentvolumes
 env:
 - name: PROVISIONER_NAME
 value: fuseim.pri/ifs
 - name: NFS_SERVER
 value: k8s-master-01
 - name: NFS_PATH
 value: /home/wjw_data/nfsdata
 volumes:
 - name: nfs-client-root
 nfs:
 server: k8s-master-01
 path: /home/wjw_data/nfsdata

 存储服务： 调用存储类的PROVISIONER(fuseim.pri/ifs）自动生成pv和pvc,注意nfs存储路径要是本地nfs的存储路径(/home/wjw_data/nfsdata)

 Pv( persisentvolum) : 配置好的一段存储
 Pvc (PersistentVolumeClaim) : 用户pod使用PV的申请声明
 pv提供存储资源(生产者)
pvc使用存储资源(消费者)
 使用pvc绑定pv
```

执行下面命令即可部署 storageClass

```shell
 kubectl apply –f class.yaml  #创建存储类
 kubectl apply –f rbac.yaml   #创建服务账户并授权
 kubectl apply –f deployment.yaml #创建存储服务
```

### 部署 kuboard

yaml 脚本为 kuboard-v3-storage-class.yaml

```shell
 ---
 apiVersion: v1
 kind: Namespace
 metadata:
   name: kuboard

 ---
 apiVersion: v1
 kind: ConfigMap
 metadata:
   name: kuboard-v3-config
   namespace: kuboard
 data:
   # 关于如下参数的解释，请参考文档 [https://kuboard.cn/install/v3/install-built-in.html](https://kuboard.cn/install/v3/install-built-in.html)
   # [common]
   KUBOARD_ENDPOINT: 'http://节点:30080'  # 节点指k8s集群节点
   KUBOARD_AGENT_SERVER_UDP_PORT: '30081'
   KUBOARD_AGENT_SERVER_TCP_PORT: '30081'
   KUBOARD_SERVER_LOGRUS_LEVEL: info  # error / debug / trace
   # KUBOARD_AGENT_KEY 是 Agent 与 Kuboard 通信时的密钥，请修改为一个任意的包含字母、数字的32位字符串，此密钥变更后，需要删除 Kuboard Agent 重新导入。
   KUBOARD_AGENT_KEY: 32b7d6572c6255211b4eec9009e4a816

   # 关于如下参数的解释，请参考文档 [https://kuboard.cn/install/v3/install-gitlab.html](https://kuboard.cn/install/v3/install-gitlab.html)
   # [gitlab login]
   # KUBOARD_LOGIN_TYPE: "gitlab"
   # KUBOARD_ROOT_USER: "your-user-name-in-gitlab"
   # GITLAB_BASE_URL: "http://gitlab.mycompany.com"
   # GITLAB_APPLICATION_ID: "7c10882aa46810a0402d17c66103894ac5e43d6130b81c17f7f2d8ae182040b5"
   # GITLAB_CLIENT_SECRET: "77c149bd3a4b6870bffa1a1afaf37cba28a1817f4cf518699065f5a8fe958889"

   # 关于如下参数的解释，请参考文档 [https://kuboard.cn/install/v3/install-github.html](https://kuboard.cn/install/v3/install-github.html)
   # [github login]
   # KUBOARD_LOGIN_TYPE: "github"
   # KUBOARD_ROOT_USER: "your-user-name-in-github"
   # GITHUB_CLIENT_ID: "17577d45e4de7dad88e0"
   # GITHUB_CLIENT_SECRET: "ff738553a8c7e9ad39569c8d02c1d85ec19115a7"

   # 关于如下参数的解释，请参考文档 [https://kuboard.cn/install/v3/install-ldap.html](https://kuboard.cn/install/v3/install-ldap.html)
   # [ldap login]
   # KUBOARD_LOGIN_TYPE: "ldap"
   # KUBOARD_ROOT_USER: "your-user-name-in-ldap"
   # LDAP_HOST: "ldap-ip-address:389"
   # LDAP_BIND_DN: "cn=admin,dc=example,dc=org"
   # LDAP_BIND_PASSWORD: "admin"
   # LDAP_BASE_DN: "dc=example,dc=org"
   # LDAP_FILTER: "(objectClass=posixAccount)"
   # LDAP_ID_ATTRIBUTE: "uid"
   # LDAP_USER_NAME_ATTRIBUTE: "uid"
   # LDAP_EMAIL_ATTRIBUTE: "mail"
   # LDAP_DISPLAY_NAME_ATTRIBUTE: "cn"
   # LDAP_GROUP_SEARCH_BASE_DN: "dc=example,dc=org"
   # LDAP_GROUP_SEARCH_FILTER: "(objectClass=posixGroup)"
   # LDAP_USER_MACHER_USER_ATTRIBUTE: "gidNumber"
   # LDAP_USER_MACHER_GROUP_ATTRIBUTE: "gidNumber"
   # LDAP_GROUP_NAME_ATTRIBUTE: "cn"

 ---
 apiVersion: apps/v1
 kind: StatefulSet
 metadata:
   name: kuboard-etcd
   namespace: kuboard
   labels:
     app: kuboard-etcd
 spec:
   serviceName: kuboard-etcd
   replicas: 3
   selector:
     matchLabels:
       app: kuboard-etcd
   template:
     metadata:
       name: kuboard-etcd
       labels:
         app: kuboard-etcd
     spec:
       containers:
       - name: kuboard-etcd
         image: swr.cn-east-2.myhuaweicloud.com/kuboard/etcd:v3.4.14
         ports:
         - containerPort: 2379
           name: client
         - containerPort: 2380
           name: peer
         env:
         - name: KUBOARD_ETCD_ENDPOINTS
           value: -
             kuboard-etcd-0.kuboard-etcd:2379,kuboard-etcd-1.kuboard-etcd:2379,kuboard-etcd-2.kuboard-etcd:2379
         volumeMounts:
         - name: data
           mountPath: /data
         command:
           - /bin/sh
           - -c
           - |
             PEERS="kuboard-etcd-0=http://kuboard-etcd-0.kuboard-etcd:2380,kuboard-etcd-1=http://kuboard-etcd-1.kuboard-etcd:2380,kuboard-etcd-2=http://kuboard-etcd-2.kuboard-etcd:2380"
             exec etcd --name ${HOSTNAME} \
               --listen-peer-urls [http://0.0.0.0:2380](http://0.0.0.0:2380) \
               --listen-client-urls [http://0.0.0.0:2379](http://0.0.0.0:2379) \
               --advertise-client-urls http://${HOSTNAME}.kuboard-etcd:2379 \
               --initial-advertise-peer-urls http://${HOSTNAME}:2380 \
               --initial-cluster-token kuboard-etcd-cluster-1 \
               --initial-cluster ${PEERS} \
               --initial-cluster-state new \
               --data-dir /data/kuboard.etcd
   volumeClaimTemplates:
   - metadata:
       name: data
     spec:
       # 请填写一个有效的 StorageClass name
       storageClassName: managed-nfs-storage
       accessModes: [ "ReadWriteMany" ]
       resources:
         requests:
           storage: 5Gi

 ---
 apiVersion: v1
 kind: Service
 metadata:
   name: kuboard-etcd
   namespace: kuboard
 spec:
   type: ClusterIP
   ports:
   - port: 2379
     name: client
   - port: 2380
     name: peer
   selector:
     app: kuboard-etcd

 ---
 apiVersion: apps/v1
 kind: Deployment
 metadata:
   annotations:
     deployment.kubernetes.io/revision: '9'
     k8s.kuboard.cn/ingress: 'false'
     k8s.kuboard.cn/service: NodePort
     k8s.kuboard.cn/workload: kuboard-v3
   labels:
     k8s.kuboard.cn/name: kuboard-v3
   name: kuboard-v3
   namespace: kuboard
 spec:
   replicas: 1
   selector:
     matchLabels:
       k8s.kuboard.cn/name: kuboard-v3
   template:
     metadata:
       labels:
         k8s.kuboard.cn/name: kuboard-v3
     spec:
       containers:
         - env:
             - name: KUBOARD_ETCD_ENDPOINTS
               value: -
                 kuboard-etcd-0.kuboard-etcd:2379,kuboard-etcd-1.kuboard-etcd:2379,kuboard-etcd-2.kuboard-etcd:2379
           envFrom:
             - configMapRef:
                 name: kuboard-v3-config
           image: 'swr.cn-east-2.myhuaweicloud.com/kuboard/kuboard:v3'
           imagePullPolicy: Always
           name: kuboard

 ---
 apiVersion: v1
 kind: Service
 metadata:
   annotations:
     k8s.kuboard.cn/workload: kuboard-v3
   labels:
     k8s.kuboard.cn/name: kuboard-v3
   name: kuboard-v3
   namespace: kuboard
 spec:
   ports:
     - name: webui
       nodePort: 30080
       port: 80
       protocol: TCP
       targetPort: 80
     - name: agentservertcp
       nodePort: 30081
       port: 10081
       protocol: TCP
       targetPort: 10081
     - name: agentserverudp
       nodePort: 30081
       port: 10081
       protocol: UDP
       targetPort: 10081
   selector:
     k8s.kuboard.cn/name: kuboard-v3
   sessionAffinity: None
   type: NodePort
```

执行下面命令就可以部署 kuboard

```shell
 Kubectl apply –f kuboard-v3-storage-class.yaml  #部署kuboard
```
